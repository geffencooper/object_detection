============================ Raw Args ============================
Namespace(batch_size=32, classification='y', dropout='n', dropout_prob=0.0, gpu_i=-1, imbalanced_sampler='n', k_folds='4', l2_reg='n', load_trained='n', log_dest='../models/activity-2021-11-20_13-30-40', loss_freq=2, lr=0.005, model_name='ActivityFCN', normalize='n', num_classes=6, num_epochs=10, optim='Adam', regression='n', root_dir='none', session_name='activity', test_data_dir='none', test_labels_csv='none', train_data_dir='none', train_labels_csv='none', trained_path='n', val_freq=0, weight_decay_amnt=0.0, weighted_loss='n')



================================ Start Training ================================

Session Name: activity

Model Name: ActivityFCN
Device: cpu

Hyperparameters:
Batch Size: 32
Learning Rate: 0.005
Number of Epochs: 10
Normalization:n


FOLD 0
=================================================================
Train Epoch: 0 Iteration: 2 [64/7352 (1%)]	 Batch 2 Loss: 2.643415
Train Epoch: 0 Iteration: 4 [128/7352 (2%)]	 Batch 4 Loss: 1.674278
Train Epoch: 0 Iteration: 6 [192/7352 (3%)]	 Batch 6 Loss: 1.336492
Train Epoch: 0 Iteration: 8 [256/7352 (5%)]	 Batch 8 Loss: 1.067024
Train Epoch: 0 Iteration: 10 [320/7352 (6%)]	 Batch 10 Loss: 1.137956
Train Epoch: 0 Iteration: 12 [384/7352 (7%)]	 Batch 12 Loss: 1.028833
Train Epoch: 0 Iteration: 14 [448/7352 (8%)]	 Batch 14 Loss: 0.920289
Train Epoch: 0 Iteration: 16 [512/7352 (9%)]	 Batch 16 Loss: 0.812462
Train Epoch: 0 Iteration: 18 [576/7352 (10%)]	 Batch 18 Loss: 0.837036
Train Epoch: 0 Iteration: 20 [640/7352 (12%)]	 Batch 20 Loss: 0.858759
Train Epoch: 0 Iteration: 22 [704/7352 (13%)]	 Batch 22 Loss: 0.736125
Train Epoch: 0 Iteration: 24 [768/7352 (14%)]	 Batch 24 Loss: 0.880794
Train Epoch: 0 Iteration: 26 [832/7352 (15%)]	 Batch 26 Loss: 0.615650
Train Epoch: 0 Iteration: 28 [896/7352 (16%)]	 Batch 28 Loss: 0.651020
Train Epoch: 0 Iteration: 30 [960/7352 (17%)]	 Batch 30 Loss: 0.628840
Train Epoch: 0 Iteration: 32 [1024/7352 (18%)]	 Batch 32 Loss: 0.568046
Train Epoch: 0 Iteration: 34 [1088/7352 (20%)]	 Batch 34 Loss: 0.524004
Train Epoch: 0 Iteration: 36 [1152/7352 (21%)]	 Batch 36 Loss: 0.669187
Train Epoch: 0 Iteration: 38 [1216/7352 (22%)]	 Batch 38 Loss: 0.698155
Train Epoch: 0 Iteration: 40 [1280/7352 (23%)]	 Batch 40 Loss: 0.531261
Train Epoch: 0 Iteration: 42 [1344/7352 (24%)]	 Batch 42 Loss: 0.392636
Train Epoch: 0 Iteration: 44 [1408/7352 (25%)]	 Batch 44 Loss: 0.333152
Train Epoch: 0 Iteration: 46 [1472/7352 (27%)]	 Batch 46 Loss: 0.427646
Train Epoch: 0 Iteration: 48 [1536/7352 (28%)]	 Batch 48 Loss: 0.355061
Train Epoch: 0 Iteration: 50 [1600/7352 (29%)]	 Batch 50 Loss: 0.473373
Train Epoch: 0 Iteration: 52 [1664/7352 (30%)]	 Batch 52 Loss: 0.372210
Train Epoch: 0 Iteration: 54 [1728/7352 (31%)]	 Batch 54 Loss: 0.325476
Train Epoch: 0 Iteration: 56 [1792/7352 (32%)]	 Batch 56 Loss: 0.272279
Train Epoch: 0 Iteration: 58 [1856/7352 (34%)]	 Batch 58 Loss: 0.486024
Train Epoch: 0 Iteration: 60 [1920/7352 (35%)]	 Batch 60 Loss: 0.428349
Train Epoch: 0 Iteration: 62 [1984/7352 (36%)]	 Batch 62 Loss: 0.495167
Train Epoch: 0 Iteration: 64 [2048/7352 (37%)]	 Batch 64 Loss: 0.374519
Train Epoch: 0 Iteration: 66 [2112/7352 (38%)]	 Batch 66 Loss: 0.475742
Train Epoch: 0 Iteration: 68 [2176/7352 (39%)]	 Batch 68 Loss: 0.398984
Train Epoch: 0 Iteration: 70 [2240/7352 (40%)]	 Batch 70 Loss: 0.270535
Train Epoch: 0 Iteration: 72 [2304/7352 (42%)]	 Batch 72 Loss: 0.205401
Train Epoch: 0 Iteration: 74 [2368/7352 (43%)]	 Batch 74 Loss: 0.428559
Train Epoch: 0 Iteration: 76 [2432/7352 (44%)]	 Batch 76 Loss: 0.220554
Train Epoch: 0 Iteration: 78 [2496/7352 (45%)]	 Batch 78 Loss: 0.302109
Train Epoch: 0 Iteration: 80 [2560/7352 (46%)]	 Batch 80 Loss: 0.258158
Train Epoch: 0 Iteration: 82 [2624/7352 (47%)]	 Batch 82 Loss: 0.285478
Train Epoch: 0 Iteration: 84 [2688/7352 (49%)]	 Batch 84 Loss: 0.264870
Train Epoch: 0 Iteration: 86 [2752/7352 (50%)]	 Batch 86 Loss: 0.247303
Train Epoch: 0 Iteration: 88 [2816/7352 (51%)]	 Batch 88 Loss: 0.265904
Train Epoch: 0 Iteration: 90 [2880/7352 (52%)]	 Batch 90 Loss: 0.389772
Train Epoch: 0 Iteration: 92 [2944/7352 (53%)]	 Batch 92 Loss: 0.266847
Train Epoch: 0 Iteration: 94 [3008/7352 (54%)]	 Batch 94 Loss: 0.489675
Train Epoch: 0 Iteration: 96 [3072/7352 (55%)]	 Batch 96 Loss: 0.206652
Train Epoch: 0 Iteration: 98 [3136/7352 (57%)]	 Batch 98 Loss: 0.278365
Train Epoch: 0 Iteration: 100 [3200/7352 (58%)]	 Batch 100 Loss: 0.430874
Train Epoch: 0 Iteration: 102 [3264/7352 (59%)]	 Batch 102 Loss: 0.277881
Train Epoch: 0 Iteration: 104 [3328/7352 (60%)]	 Batch 104 Loss: 0.313840
Train Epoch: 0 Iteration: 106 [3392/7352 (61%)]	 Batch 106 Loss: 0.228007
Train Epoch: 0 Iteration: 108 [3456/7352 (62%)]	 Batch 108 Loss: 0.339081
Train Epoch: 0 Iteration: 110 [3520/7352 (64%)]	 Batch 110 Loss: 0.241470
Train Epoch: 0 Iteration: 112 [3584/7352 (65%)]	 Batch 112 Loss: 0.217045
Train Epoch: 0 Iteration: 114 [3648/7352 (66%)]	 Batch 114 Loss: 0.444854
Train Epoch: 0 Iteration: 116 [3712/7352 (67%)]	 Batch 116 Loss: 0.382719
Train Epoch: 0 Iteration: 118 [3776/7352 (68%)]	 Batch 118 Loss: 0.213932
Train Epoch: 0 Iteration: 120 [3840/7352 (69%)]	 Batch 120 Loss: 0.326005
Train Epoch: 0 Iteration: 122 [3904/7352 (71%)]	 Batch 122 Loss: 0.168055
Train Epoch: 0 Iteration: 124 [3968/7352 (72%)]	 Batch 124 Loss: 0.265786
Train Epoch: 0 Iteration: 126 [4032/7352 (73%)]	 Batch 126 Loss: 0.299003
Train Epoch: 0 Iteration: 128 [4096/7352 (74%)]	 Batch 128 Loss: 0.201492
Train Epoch: 0 Iteration: 130 [4160/7352 (75%)]	 Batch 130 Loss: 0.247779
Train Epoch: 0 Iteration: 132 [4224/7352 (76%)]	 Batch 132 Loss: 0.269986
Train Epoch: 0 Iteration: 134 [4288/7352 (77%)]	 Batch 134 Loss: 0.272692
Train Epoch: 0 Iteration: 136 [4352/7352 (79%)]	 Batch 136 Loss: 0.234459
Train Epoch: 0 Iteration: 138 [4416/7352 (80%)]	 Batch 138 Loss: 0.203711
Train Epoch: 0 Iteration: 140 [4480/7352 (81%)]	 Batch 140 Loss: 0.327100
Train Epoch: 0 Iteration: 142 [4544/7352 (82%)]	 Batch 142 Loss: 0.218367
Train Epoch: 0 Iteration: 144 [4608/7352 (83%)]	 Batch 144 Loss: 0.355849
Train Epoch: 0 Iteration: 146 [4672/7352 (84%)]	 Batch 146 Loss: 0.214468
Train Epoch: 0 Iteration: 148 [4736/7352 (86%)]	 Batch 148 Loss: 0.292872
Train Epoch: 0 Iteration: 150 [4800/7352 (87%)]	 Batch 150 Loss: 0.167868
Train Epoch: 0 Iteration: 152 [4864/7352 (88%)]	 Batch 152 Loss: 0.308995
Train Epoch: 0 Iteration: 154 [4928/7352 (89%)]	 Batch 154 Loss: 0.234465
Train Epoch: 0 Iteration: 156 [4992/7352 (90%)]	 Batch 156 Loss: 0.251479
Train Epoch: 0 Iteration: 158 [5056/7352 (91%)]	 Batch 158 Loss: 0.183648
Train Epoch: 0 Iteration: 160 [5120/7352 (92%)]	 Batch 160 Loss: 0.194411
Train Epoch: 0 Iteration: 162 [5184/7352 (94%)]	 Batch 162 Loss: 0.167590
Train Epoch: 0 Iteration: 164 [5248/7352 (95%)]	 Batch 164 Loss: 0.102485
Train Epoch: 0 Iteration: 166 [5312/7352 (96%)]	 Batch 166 Loss: 0.221839
Train Epoch: 0 Iteration: 168 [5376/7352 (97%)]	 Batch 168 Loss: 0.240337
Train Epoch: 0 Iteration: 170 [5440/7352 (98%)]	 Batch 170 Loss: 0.135628
Train Epoch: 0 Iteration: 172 [5504/7352 (99%)]	 Batch 172 Loss: 0.084733


----------------- Epoch 0 -----------------

1838
validation computation time: 0.0  minutes
Confusion Matrix
tensor([[300,  61,   0,   0,   0,   0],
        [ 27, 271,   3,   0,   0,   0],
        [  0,   2, 344,   0,   0,   0],
        [  0,   0,   0, 274,   0,   0],
        [  0,   0,   0,   3, 244,   6],
        [  0,   0,   1,  13,   7, 282]])
class 0 accuracy: 91.7431%
class 1 accuracy: 81.1377%
class 2 accuracy: 98.8506%
class 3 accuracy: 94.4828%
class 4 accuracy: 97.2112%
class 5 accuracy: 97.9167%

Validation Loss: 0.1900, Accuracy: 1715/1838 (93%)
Training Loss:0.4637
Best Accuracy: 93.307943%
Time Elapsed: 0h 0m 15s

--------------------------------------------------------


Train Epoch: 1 Iteration: 2 [64/7352 (1%)]	 Batch 2 Loss: 0.298261
Train Epoch: 1 Iteration: 4 [128/7352 (2%)]	 Batch 4 Loss: 0.147922
Train Epoch: 1 Iteration: 6 [192/7352 (3%)]	 Batch 6 Loss: 0.139508
Train Epoch: 1 Iteration: 8 [256/7352 (5%)]	 Batch 8 Loss: 0.133936
Train Epoch: 1 Iteration: 10 [320/7352 (6%)]	 Batch 10 Loss: 0.211441
Train Epoch: 1 Iteration: 12 [384/7352 (7%)]	 Batch 12 Loss: 0.364193
Train Epoch: 1 Iteration: 14 [448/7352 (8%)]	 Batch 14 Loss: 0.112701
Train Epoch: 1 Iteration: 16 [512/7352 (9%)]	 Batch 16 Loss: 0.226342
Train Epoch: 1 Iteration: 18 [576/7352 (10%)]	 Batch 18 Loss: 0.400010
Train Epoch: 1 Iteration: 20 [640/7352 (12%)]	 Batch 20 Loss: 0.157842
Train Epoch: 1 Iteration: 22 [704/7352 (13%)]	 Batch 22 Loss: 0.345626
Train Epoch: 1 Iteration: 24 [768/7352 (14%)]	 Batch 24 Loss: 0.164698
Train Epoch: 1 Iteration: 26 [832/7352 (15%)]	 Batch 26 Loss: 0.318480
Train Epoch: 1 Iteration: 28 [896/7352 (16%)]	 Batch 28 Loss: 0.170000
Train Epoch: 1 Iteration: 30 [960/7352 (17%)]	 Batch 30 Loss: 0.204469
Train Epoch: 1 Iteration: 32 [1024/7352 (18%)]	 Batch 32 Loss: 0.145883
Train Epoch: 1 Iteration: 34 [1088/7352 (20%)]	 Batch 34 Loss: 0.101932
Train Epoch: 1 Iteration: 36 [1152/7352 (21%)]	 Batch 36 Loss: 0.186838
Train Epoch: 1 Iteration: 38 [1216/7352 (22%)]	 Batch 38 Loss: 0.157616
Train Epoch: 1 Iteration: 40 [1280/7352 (23%)]	 Batch 40 Loss: 0.112422
Train Epoch: 1 Iteration: 42 [1344/7352 (24%)]	 Batch 42 Loss: 0.148903
Train Epoch: 1 Iteration: 44 [1408/7352 (25%)]	 Batch 44 Loss: 0.105464
Train Epoch: 1 Iteration: 46 [1472/7352 (27%)]	 Batch 46 Loss: 0.091685
Train Epoch: 1 Iteration: 48 [1536/7352 (28%)]	 Batch 48 Loss: 0.150600
Train Epoch: 1 Iteration: 50 [1600/7352 (29%)]	 Batch 50 Loss: 0.107198
Train Epoch: 1 Iteration: 52 [1664/7352 (30%)]	 Batch 52 Loss: 0.114621
Train Epoch: 1 Iteration: 54 [1728/7352 (31%)]	 Batch 54 Loss: 0.146370
Train Epoch: 1 Iteration: 56 [1792/7352 (32%)]	 Batch 56 Loss: 0.153169
Train Epoch: 1 Iteration: 58 [1856/7352 (34%)]	 Batch 58 Loss: 0.166515
Train Epoch: 1 Iteration: 60 [1920/7352 (35%)]	 Batch 60 Loss: 0.170007
Train Epoch: 1 Iteration: 62 [1984/7352 (36%)]	 Batch 62 Loss: 0.103550
Train Epoch: 1 Iteration: 64 [2048/7352 (37%)]	 Batch 64 Loss: 0.129585
Train Epoch: 1 Iteration: 66 [2112/7352 (38%)]	 Batch 66 Loss: 0.108978
Train Epoch: 1 Iteration: 68 [2176/7352 (39%)]	 Batch 68 Loss: 0.063758
Train Epoch: 1 Iteration: 70 [2240/7352 (40%)]	 Batch 70 Loss: 0.128343
Train Epoch: 1 Iteration: 72 [2304/7352 (42%)]	 Batch 72 Loss: 0.095136
Train Epoch: 1 Iteration: 74 [2368/7352 (43%)]	 Batch 74 Loss: 0.128146
Train Epoch: 1 Iteration: 76 [2432/7352 (44%)]	 Batch 76 Loss: 0.155033
Train Epoch: 1 Iteration: 78 [2496/7352 (45%)]	 Batch 78 Loss: 0.181406
Train Epoch: 1 Iteration: 80 [2560/7352 (46%)]	 Batch 80 Loss: 0.115417
Train Epoch: 1 Iteration: 82 [2624/7352 (47%)]	 Batch 82 Loss: 0.122423
Train Epoch: 1 Iteration: 84 [2688/7352 (49%)]	 Batch 84 Loss: 0.197310
Train Epoch: 1 Iteration: 86 [2752/7352 (50%)]	 Batch 86 Loss: 0.140239
Train Epoch: 1 Iteration: 88 [2816/7352 (51%)]	 Batch 88 Loss: 0.179531
Train Epoch: 1 Iteration: 90 [2880/7352 (52%)]	 Batch 90 Loss: 0.142687
Train Epoch: 1 Iteration: 92 [2944/7352 (53%)]	 Batch 92 Loss: 0.215826
Train Epoch: 1 Iteration: 94 [3008/7352 (54%)]	 Batch 94 Loss: 0.114717
Train Epoch: 1 Iteration: 96 [3072/7352 (55%)]	 Batch 96 Loss: 0.063562
Train Epoch: 1 Iteration: 98 [3136/7352 (57%)]	 Batch 98 Loss: 0.282787
Train Epoch: 1 Iteration: 100 [3200/7352 (58%)]	 Batch 100 Loss: 0.080535
Train Epoch: 1 Iteration: 102 [3264/7352 (59%)]	 Batch 102 Loss: 0.149151
Train Epoch: 1 Iteration: 104 [3328/7352 (60%)]	 Batch 104 Loss: 0.154972
Train Epoch: 1 Iteration: 106 [3392/7352 (61%)]	 Batch 106 Loss: 0.265448
Train Epoch: 1 Iteration: 108 [3456/7352 (62%)]	 Batch 108 Loss: 0.166210
Train Epoch: 1 Iteration: 110 [3520/7352 (64%)]	 Batch 110 Loss: 0.222161
Train Epoch: 1 Iteration: 112 [3584/7352 (65%)]	 Batch 112 Loss: 0.119977
Train Epoch: 1 Iteration: 114 [3648/7352 (66%)]	 Batch 114 Loss: 0.195111
Train Epoch: 1 Iteration: 116 [3712/7352 (67%)]	 Batch 116 Loss: 0.175128
Train Epoch: 1 Iteration: 118 [3776/7352 (68%)]	 Batch 118 Loss: 0.181330
Train Epoch: 1 Iteration: 120 [3840/7352 (69%)]	 Batch 120 Loss: 0.149787
Train Epoch: 1 Iteration: 122 [3904/7352 (71%)]	 Batch 122 Loss: 0.150860
Train Epoch: 1 Iteration: 124 [3968/7352 (72%)]	 Batch 124 Loss: 0.143431
Train Epoch: 1 Iteration: 126 [4032/7352 (73%)]	 Batch 126 Loss: 0.064133
Train Epoch: 1 Iteration: 128 [4096/7352 (74%)]	 Batch 128 Loss: 0.192302
Train Epoch: 1 Iteration: 130 [4160/7352 (75%)]	 Batch 130 Loss: 0.135621
Train Epoch: 1 Iteration: 132 [4224/7352 (76%)]	 Batch 132 Loss: 0.206698
Train Epoch: 1 Iteration: 134 [4288/7352 (77%)]	 Batch 134 Loss: 0.075303
Train Epoch: 1 Iteration: 136 [4352/7352 (79%)]	 Batch 136 Loss: 0.203488
Train Epoch: 1 Iteration: 138 [4416/7352 (80%)]	 Batch 138 Loss: 0.179513
Train Epoch: 1 Iteration: 140 [4480/7352 (81%)]	 Batch 140 Loss: 0.190931
Train Epoch: 1 Iteration: 142 [4544/7352 (82%)]	 Batch 142 Loss: 0.196886
Train Epoch: 1 Iteration: 144 [4608/7352 (83%)]	 Batch 144 Loss: 0.139800
Train Epoch: 1 Iteration: 146 [4672/7352 (84%)]	 Batch 146 Loss: 0.154880
Train Epoch: 1 Iteration: 148 [4736/7352 (86%)]	 Batch 148 Loss: 0.242693
Train Epoch: 1 Iteration: 150 [4800/7352 (87%)]	 Batch 150 Loss: 0.085795
Train Epoch: 1 Iteration: 152 [4864/7352 (88%)]	 Batch 152 Loss: 0.157242
Train Epoch: 1 Iteration: 154 [4928/7352 (89%)]	 Batch 154 Loss: 0.069006
Train Epoch: 1 Iteration: 156 [4992/7352 (90%)]	 Batch 156 Loss: 0.039872
Train Epoch: 1 Iteration: 158 [5056/7352 (91%)]	 Batch 158 Loss: 0.130276
Train Epoch: 1 Iteration: 160 [5120/7352 (92%)]	 Batch 160 Loss: 0.176490
Train Epoch: 1 Iteration: 162 [5184/7352 (94%)]	 Batch 162 Loss: 0.044281
Train Epoch: 1 Iteration: 164 [5248/7352 (95%)]	 Batch 164 Loss: 0.075515
Train Epoch: 1 Iteration: 166 [5312/7352 (96%)]	 Batch 166 Loss: 0.124391
Train Epoch: 1 Iteration: 168 [5376/7352 (97%)]	 Batch 168 Loss: 0.158167
Train Epoch: 1 Iteration: 170 [5440/7352 (98%)]	 Batch 170 Loss: 0.210543
Train Epoch: 1 Iteration: 172 [5504/7352 (99%)]	 Batch 172 Loss: 0.255288


----------------- Epoch 1 -----------------

1838
validation computation time: 0.0  minutes
Confusion Matrix
tensor([[312,  38,   0,   0,   0,   0],
        [ 15, 296,   6,   0,   0,   0],
        [  0,   0, 342,   0,   0,   0],
        [  0,   0,   0, 287,   4,   2],
        [  0,   0,   0,   1, 242,   1],
        [  0,   0,   0,   2,   5, 285]])
class 0 accuracy: 95.4128%
class 1 accuracy: 88.6228%
class 2 accuracy: 98.2759%
class 3 accuracy: 98.9655%
class 4 accuracy: 96.4143%
class 5 accuracy: 98.9583%

Validation Loss: 0.1172, Accuracy: 1764/1838 (96%)
Training Loss:0.1581
Best Accuracy: 95.973885%
Time Elapsed: 0h 0m 31s

--------------------------------------------------------


Train Epoch: 2 Iteration: 2 [64/7352 (1%)]	 Batch 2 Loss: 0.136891
Train Epoch: 2 Iteration: 4 [128/7352 (2%)]	 Batch 4 Loss: 0.112828
Train Epoch: 2 Iteration: 6 [192/7352 (3%)]	 Batch 6 Loss: 0.171397
Train Epoch: 2 Iteration: 8 [256/7352 (5%)]	 Batch 8 Loss: 0.089278
Train Epoch: 2 Iteration: 10 [320/7352 (6%)]	 Batch 10 Loss: 0.077754
Train Epoch: 2 Iteration: 12 [384/7352 (7%)]	 Batch 12 Loss: 0.075900
Train Epoch: 2 Iteration: 14 [448/7352 (8%)]	 Batch 14 Loss: 0.137887
Train Epoch: 2 Iteration: 16 [512/7352 (9%)]	 Batch 16 Loss: 0.109809
Train Epoch: 2 Iteration: 18 [576/7352 (10%)]	 Batch 18 Loss: 0.046364
Train Epoch: 2 Iteration: 20 [640/7352 (12%)]	 Batch 20 Loss: 0.150144
Train Epoch: 2 Iteration: 22 [704/7352 (13%)]	 Batch 22 Loss: 0.110580
Train Epoch: 2 Iteration: 24 [768/7352 (14%)]	 Batch 24 Loss: 0.091467
Train Epoch: 2 Iteration: 26 [832/7352 (15%)]	 Batch 26 Loss: 0.202596
Train Epoch: 2 Iteration: 28 [896/7352 (16%)]	 Batch 28 Loss: 0.121662
Train Epoch: 2 Iteration: 30 [960/7352 (17%)]	 Batch 30 Loss: 0.095671
Train Epoch: 2 Iteration: 32 [1024/7352 (18%)]	 Batch 32 Loss: 0.156318
Train Epoch: 2 Iteration: 34 [1088/7352 (20%)]	 Batch 34 Loss: 0.068038
Train Epoch: 2 Iteration: 36 [1152/7352 (21%)]	 Batch 36 Loss: 0.073305
Train Epoch: 2 Iteration: 38 [1216/7352 (22%)]	 Batch 38 Loss: 0.263735
Train Epoch: 2 Iteration: 40 [1280/7352 (23%)]	 Batch 40 Loss: 0.092047
Train Epoch: 2 Iteration: 42 [1344/7352 (24%)]	 Batch 42 Loss: 0.104978
Train Epoch: 2 Iteration: 44 [1408/7352 (25%)]	 Batch 44 Loss: 0.089123
Train Epoch: 2 Iteration: 46 [1472/7352 (27%)]	 Batch 46 Loss: 0.236277
Train Epoch: 2 Iteration: 48 [1536/7352 (28%)]	 Batch 48 Loss: 0.100676
Train Epoch: 2 Iteration: 50 [1600/7352 (29%)]	 Batch 50 Loss: 0.092469
Train Epoch: 2 Iteration: 52 [1664/7352 (30%)]	 Batch 52 Loss: 0.147006
Train Epoch: 2 Iteration: 54 [1728/7352 (31%)]	 Batch 54 Loss: 0.051924
Train Epoch: 2 Iteration: 56 [1792/7352 (32%)]	 Batch 56 Loss: 0.295090
Train Epoch: 2 Iteration: 58 [1856/7352 (34%)]	 Batch 58 Loss: 0.068576
Train Epoch: 2 Iteration: 60 [1920/7352 (35%)]	 Batch 60 Loss: 0.152262
Train Epoch: 2 Iteration: 62 [1984/7352 (36%)]	 Batch 62 Loss: 0.050045
Train Epoch: 2 Iteration: 64 [2048/7352 (37%)]	 Batch 64 Loss: 0.145463
Train Epoch: 2 Iteration: 66 [2112/7352 (38%)]	 Batch 66 Loss: 0.530511
Train Epoch: 2 Iteration: 68 [2176/7352 (39%)]	 Batch 68 Loss: 0.067576
Train Epoch: 2 Iteration: 70 [2240/7352 (40%)]	 Batch 70 Loss: 0.280199
Train Epoch: 2 Iteration: 72 [2304/7352 (42%)]	 Batch 72 Loss: 0.085852
Train Epoch: 2 Iteration: 74 [2368/7352 (43%)]	 Batch 74 Loss: 0.049256
Train Epoch: 2 Iteration: 76 [2432/7352 (44%)]	 Batch 76 Loss: 0.065142
Train Epoch: 2 Iteration: 78 [2496/7352 (45%)]	 Batch 78 Loss: 0.123968
Train Epoch: 2 Iteration: 80 [2560/7352 (46%)]	 Batch 80 Loss: 0.051865
Train Epoch: 2 Iteration: 82 [2624/7352 (47%)]	 Batch 82 Loss: 0.093327
Train Epoch: 2 Iteration: 84 [2688/7352 (49%)]	 Batch 84 Loss: 0.172116
Train Epoch: 2 Iteration: 86 [2752/7352 (50%)]	 Batch 86 Loss: 0.126070
Train Epoch: 2 Iteration: 88 [2816/7352 (51%)]	 Batch 88 Loss: 0.367889
Train Epoch: 2 Iteration: 90 [2880/7352 (52%)]	 Batch 90 Loss: 0.055932
Train Epoch: 2 Iteration: 92 [2944/7352 (53%)]	 Batch 92 Loss: 0.213814
Train Epoch: 2 Iteration: 94 [3008/7352 (54%)]	 Batch 94 Loss: 0.090275
Train Epoch: 2 Iteration: 96 [3072/7352 (55%)]	 Batch 96 Loss: 0.232411
Train Epoch: 2 Iteration: 98 [3136/7352 (57%)]	 Batch 98 Loss: 0.019066
Train Epoch: 2 Iteration: 100 [3200/7352 (58%)]	 Batch 100 Loss: 0.133727
Train Epoch: 2 Iteration: 102 [3264/7352 (59%)]	 Batch 102 Loss: 0.046986
Train Epoch: 2 Iteration: 104 [3328/7352 (60%)]	 Batch 104 Loss: 0.148976
Train Epoch: 2 Iteration: 106 [3392/7352 (61%)]	 Batch 106 Loss: 0.126078
Train Epoch: 2 Iteration: 108 [3456/7352 (62%)]	 Batch 108 Loss: 0.074196
Train Epoch: 2 Iteration: 110 [3520/7352 (64%)]	 Batch 110 Loss: 0.063785
Train Epoch: 2 Iteration: 112 [3584/7352 (65%)]	 Batch 112 Loss: 0.063843
Train Epoch: 2 Iteration: 114 [3648/7352 (66%)]	 Batch 114 Loss: 0.048062
Train Epoch: 2 Iteration: 116 [3712/7352 (67%)]	 Batch 116 Loss: 0.027892
Train Epoch: 2 Iteration: 118 [3776/7352 (68%)]	 Batch 118 Loss: 0.042839
Train Epoch: 2 Iteration: 120 [3840/7352 (69%)]	 Batch 120 Loss: 0.038184
Train Epoch: 2 Iteration: 122 [3904/7352 (71%)]	 Batch 122 Loss: 0.080388
Train Epoch: 2 Iteration: 124 [3968/7352 (72%)]	 Batch 124 Loss: 0.066853
Train Epoch: 2 Iteration: 126 [4032/7352 (73%)]	 Batch 126 Loss: 0.043179
Train Epoch: 2 Iteration: 128 [4096/7352 (74%)]	 Batch 128 Loss: 0.084443
Train Epoch: 2 Iteration: 130 [4160/7352 (75%)]	 Batch 130 Loss: 0.082009
Train Epoch: 2 Iteration: 132 [4224/7352 (76%)]	 Batch 132 Loss: 0.014099
Train Epoch: 2 Iteration: 134 [4288/7352 (77%)]	 Batch 134 Loss: 0.087454
Train Epoch: 2 Iteration: 136 [4352/7352 (79%)]	 Batch 136 Loss: 0.061422
Train Epoch: 2 Iteration: 138 [4416/7352 (80%)]	 Batch 138 Loss: 0.056607
Train Epoch: 2 Iteration: 140 [4480/7352 (81%)]	 Batch 140 Loss: 0.191677
Train Epoch: 2 Iteration: 142 [4544/7352 (82%)]	 Batch 142 Loss: 0.108420
Train Epoch: 2 Iteration: 144 [4608/7352 (83%)]	 Batch 144 Loss: 0.131995
Train Epoch: 2 Iteration: 146 [4672/7352 (84%)]	 Batch 146 Loss: 0.154630
Train Epoch: 2 Iteration: 148 [4736/7352 (86%)]	 Batch 148 Loss: 0.075257
Train Epoch: 2 Iteration: 150 [4800/7352 (87%)]	 Batch 150 Loss: 0.082609
Train Epoch: 2 Iteration: 152 [4864/7352 (88%)]	 Batch 152 Loss: 0.055274
Train Epoch: 2 Iteration: 154 [4928/7352 (89%)]	 Batch 154 Loss: 0.049585
Train Epoch: 2 Iteration: 156 [4992/7352 (90%)]	 Batch 156 Loss: 0.245874
Train Epoch: 2 Iteration: 158 [5056/7352 (91%)]	 Batch 158 Loss: 0.076241
Train Epoch: 2 Iteration: 160 [5120/7352 (92%)]	 Batch 160 Loss: 0.073499
Train Epoch: 2 Iteration: 162 [5184/7352 (94%)]	 Batch 162 Loss: 0.055526
Train Epoch: 2 Iteration: 164 [5248/7352 (95%)]	 Batch 164 Loss: 0.077945
Train Epoch: 2 Iteration: 166 [5312/7352 (96%)]	 Batch 166 Loss: 0.087641
Train Epoch: 2 Iteration: 168 [5376/7352 (97%)]	 Batch 168 Loss: 0.106195
Train Epoch: 2 Iteration: 170 [5440/7352 (98%)]	 Batch 170 Loss: 0.105941
Train Epoch: 2 Iteration: 172 [5504/7352 (99%)]	 Batch 172 Loss: 0.029675


----------------- Epoch 2 -----------------

1838
================================ QUIT ================================
 Saving Model ...
1838
